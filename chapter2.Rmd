# Chapter 2. Analysis of the Learning 2014 dataset

## 1) Load, explore, explain

```{r message=FALSE, warning=FALSE}

library(tidyverse)
library(GGally)

tb_students <- read_delim("data/data-learning2014.txt")
str(tb_students)

```

The dataset is the result of a survey of participants in the course *Johdatus yhteiskuntatilastotieteeseen* (*Introduction to Social Statistics*), fall 2014, and surveys the participants' approaches to learning ([ASSIST](http://www.etl.tla.ed.ac.uk/publications.html#measurement), section B), attitudes toward statistics (Based on [SATS]()<https://www.evaluationandstatistics.com/scoring>), as well as their points from their exams.

Info about the study: [Metadata 1](https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS2-meta.txt) [Metadata 2](https://www.mv.helsinki.fi/home/kvehkala/JYTmooc/JYTOPKYS3-meta.txt) [PPT presentation](https://www.slideshare.net/kimmovehkalahti/the-relationship-between-learning-approaches-and-students-achievements-in-an-introductory-statistics-course-in-finland)

183 of the course's 250 students participated in the study. However, due to some missing data, there's 166 participants in the final dataset.

As we can see from **str(tb_students)** above, The dataset has 166 observations of 7 variables. Each row is one participant and for each participant we know the age (numeric), gender (F/M), their exam points (num), and their scores (numeric) on four dimensions: attitude towards statistics (attitude), Deep learning approach (deep), Surface learning approach (surf), and Strategic learning approach (strategic). These scores are based several questions in the questionnaire where the students were asked to rate (from 1 to 5) how much they agreed to certain statements, for example "I usually set out to understand for myself the meaning of what we have to learn".

## 2) Graphical overview, summary and comments

Let's have a quick look at some summaries of the data:

```{r message=FALSE, warning=FALSE}
ggpairs(
      data = tb_students %>% relocate(-gender),
      lower = list(continuous = wrap("points", alpha = 0.3, size=0.3), mapping=aes(color=gender)),
      )+
  scale_color_manual(values=c("F"="RED","M"="BLUE"))+
  scale_fill_manual(values=c("F"="RED","M"="BLUE"))


```

**Blue** represents male participants, and **red** female participants.

This plot shows us the distribution of the different variables (diagonal), the plotted relationships between the variables (lower triangle), and the measured correlations between them (upper triangle). Correlations are not shown for gender since it's a categorical variable.

From the **top left plot** we can see that most of the participants are aged around 20-30 years old, but that there are some participants in the older age groups slightly above 50 years. From the other diagonals we can see that most variables are fairly normally distributed.

There are also far more female than male participants **(bottom right plot)**; We can get some more details on that using a summary table:

```{r message=FALSE, warning=FALSE}
tb_students %>% group_by(gender) %>% summarise(n=n(), p=n() / nrow(.))
```

Which shows us that 66% (n=110) of the participants were female, and only 33% (n=56) male. However, looking at the **rightmost column** of the plot above, we can see that most variables don't seem to associate strongly with gender, except maybe attitude, where the male participants are maybe scoring slightly higher. This can also be seen on the **second plot of the bottom row**, where it seems like for the female participants there is a higher amount of participants with a lower attitude score

Looking at the correlations, the strongest one (\*\*\*) seems to be between points and attitude, where participants with a higher attitude-score seems to score more points at the exams. Surface learning also seems to correlate negatively with deep learning and strategic learning, as well as correlating negatively with attitude.

## 3) Fitting a regression model, summary, and comment statistical tests

Let's fit a regression model to examine some of the patterns more closely. We'll use the variables attidue, age, and gender

```{r message=FALSE, warning=FALSE}

model_full <- lm(points ~ attitude + age + gender, data=tb_students)
summary(model_full)

```

It seems here that only attitude has a significant association with exam points, as the t-tests of the slope estimates only give a value below 0.05 for attitude. This statistical test checks whether the slope (estimate) of an explanatory variable is significantly different from 0. The idea here is that if the slope is 0, it will randomly deviate from 0 based on the t-distribution. Using the t-test we can check if the slope is more different from 0 than what is likely for t-distribution.

In our case, only the slope for attitude is significantly different from zero; If age or gender has an effect on exam points, we're not able to detect it here.

We'll simplify the model by removing the non-significant explanatory variables:

```{r message=FALSE, warning=FALSE}
model_simple <- lm(points ~ attitude, data=tb_students)
```

## 4) Summary of simplified model, explain relationship, explain multuple R-squared

```{r message=FALSE, warning=FALSE}
summary(model_simple)
```

Our estimated slope for attitude is 0.35. This means that for each extra point on the attitude score, the expected number of exam points goes up by 0.35. The multiple R-square value (coefficient of determination) of 0.19 tells us that our explanatory variable attitude is explaning 19% of the variation in the target variable exam points.

## 5) Diagnostic plots

A key assumption for a basic liner model is that the *residuals* of the observed variable (points) are normally distributed

There are many ways of checking this, two are shown below: 1) just plot a histogram of the residuals. 2) plot a q-q plot. These two plots basically tell us the same thing, but the q-q plot makes it a little easier how much our residuals are straying from normal.

```{r message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
hist(residuals(model_simple),breaks=50)
plot(model_simple,which=2)

```

Our histogram and Q-Q plot tells us that our residuals are fairly normally distributed, but there are some extreme values towards the negative end.

Side not: Personally, I initially found it a little tricky to wrap my head around what is meant by "standardised residuals" and "theoretical quantiles", so I did some googling and tried making the q-q plot from scratch. I understand it now but it's actually quite tricky to explain with words; Anyways, here's the R code for a q-q plot from scratch:

```{r message=FALSE, warning=FALSE}
plot(sort(qnorm(seq(0,1,length.out=nrow(tb_students)))), sort(scale(residuals(model_simple))))
abline(coef=c(0,1))
```

Another important assumptions of the linear model is **homoscedasticity** (as opposed to heteroscedastisity), which means that the variance of our residuals stay the same independent of the explanatory variable (there should not be higher variance among students with a high attitude). We can check this with a residuals vs fitted plot:

```{r message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(model_simple,which=1)
plot(tb_students$attitude, residuals(model_simple),)
abline(coef=c(0,0))

```

The plot shows us that the residuals stay more or less the same independent of the "fitted values" (explanatory variable). which is good! It could look like there's less variance among the students with a high attitude-score, but this is likely just because there's fewer students there, so this is probably not a problem)

Also, to help understand the content of the plot, I've plotted the same plot twice, once using the built in residuals vs fitted plot (right), and once by making the plot from "scratch" (right)

Finally, we might want to check for extreme outliers. **outliers** are data points with an extremely high residual value. Data points with a high **leverage** are points that have a high influence on the model. Points that both have a high residual and leverage are potentially bad for the accuracy of our regression.

```{r message=FALSE, warning=FALSE}
plot(model_simple,which=5)
```

In our case, it looks like there's no problematic points as none of them are outside of **Cook's distance**, which is so far way here that it is not even shown on the plot.

**Overall, it looks like our model fits reasonably well, although we should keep the negatively skewed residuals in mind.**
